{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpatialTransformers_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tS5VJBe-iprT"
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Ghassen Hamrouni\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the  CIFAR Data"
      ],
      "metadata": {
        "id": "jLF7jcgWjN1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='.', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.49139968, 0.48215841, 0.44653091),(0.24703223, 0.24348513, 0.26158784))\n",
        "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
        "# Test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='.', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2434, 0.2615))\n",
        "    ])), batch_size=64, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBnRz9TljP4m",
        "outputId": "4065ffd3-4e42-4df3-b549-72402c108987"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "4BG4lfLxkVnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(32*5*5, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 128, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 3 * 2)\n",
        "        )\n",
        "\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, xs.size(1) * xs.size(2) * xs.size(3))\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # Perform the usual forward pass\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 32*5*5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = Net().to(device)"
      ],
      "metadata": {
        "id": "zVq2YGwIkYjo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "bsEUH64ukqpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "#\n",
        "# A simple test procedure to measure STN the performances on CIFAR.\n",
        "#\n",
        "\n",
        "\n",
        "def test():\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(test_loss, correct, len(test_loader.dataset),\n",
        "                      100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "FqymEV0Vkt2c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show the model"
      ],
      "metadata": {
        "id": "fe7i1gzXk96h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avqv6Nq_k_xR",
        "outputId": "e5ca35d2-e839-4d1f-cafd-be48d4447af9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=800, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
            "  (localization): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (fc_loc): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=256, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZING THE STN RESULTS"
      ],
      "metadata": {
        "id": "PjqilinplFEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_image_np(inp):\n",
        "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.49139968, 0.48215841, 0.44653091])\n",
        "    std = np.array([0.24703223, 0.24348513, 0.26158784])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    return inp\n",
        "\n",
        "# We want to visualize the output of the spatial transformers layer\n",
        "# after the training, we visualize a batch of input images and\n",
        "# the corresponding transformed batch using STN.\n",
        "\n",
        "\n",
        "def visualize_stn():\n",
        "    with torch.no_grad():\n",
        "        # Get a batch of training data\n",
        "        data = next(iter(test_loader))[0].to(device)\n",
        "\n",
        "        input_tensor = data.cpu()\n",
        "        transformed_input_tensor = model.stn(data).cpu()\n",
        "\n",
        "        in_grid = convert_image_np(\n",
        "            torchvision.utils.make_grid(input_tensor))\n",
        "\n",
        "        out_grid = convert_image_np(\n",
        "            torchvision.utils.make_grid(transformed_input_tensor))\n",
        "\n",
        "        # Plot the results side-by-side\n",
        "        f, axarr = plt.subplots(1, 2)\n",
        "        axarr[0].imshow(in_grid)\n",
        "        axarr[0].set_title('Dataset Images')\n",
        "\n",
        "        axarr[1].imshow(out_grid)\n",
        "        axarr[1].set_title('Transformed Images')\n",
        "\n",
        "for epoch in range(1, 50 + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "# Visualize the STN transformation on some input batch\n",
        "visualize_stn()\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hwetyAFlNGG",
        "outputId": "c3d10c9d-0226-4e70-99ba-460c78fc3117"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4066: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.175210\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.931774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7622, Accuracy: 3752/10000 (38%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.961784\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.760729\n",
            "\n",
            "Test set: Average loss: 1.5941, Accuracy: 4224/10000 (42%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.978295\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.690157\n",
            "\n",
            "Test set: Average loss: 1.5088, Accuracy: 4626/10000 (46%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.562305\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.621155\n",
            "\n",
            "Test set: Average loss: 1.4854, Accuracy: 4739/10000 (47%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.785641\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.662556\n",
            "\n",
            "Test set: Average loss: 1.4424, Accuracy: 4903/10000 (49%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.459225\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.579494\n",
            "\n",
            "Test set: Average loss: 1.3626, Accuracy: 5080/10000 (51%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.319066\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.295658\n",
            "\n",
            "Test set: Average loss: 1.3324, Accuracy: 5218/10000 (52%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.488322\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 1.540503\n",
            "\n",
            "Test set: Average loss: 1.2891, Accuracy: 5362/10000 (54%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.427897\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.209769\n",
            "\n",
            "Test set: Average loss: 1.2635, Accuracy: 5548/10000 (55%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.198874\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.402896\n",
            "\n",
            "Test set: Average loss: 1.2494, Accuracy: 5611/10000 (56%)\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.411324\n",
            "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1.343977\n",
            "\n",
            "Test set: Average loss: 1.2126, Accuracy: 5771/10000 (58%)\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.315648\n",
            "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1.157056\n",
            "\n",
            "Test set: Average loss: 1.1730, Accuracy: 5875/10000 (59%)\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.527717\n",
            "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 0.959066\n",
            "\n",
            "Test set: Average loss: 1.1599, Accuracy: 5983/10000 (60%)\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.195265\n",
            "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1.227991\n",
            "\n",
            "Test set: Average loss: 1.1382, Accuracy: 6141/10000 (61%)\n",
            "\n",
            "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.280621\n",
            "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1.262416\n",
            "\n",
            "Test set: Average loss: 1.1140, Accuracy: 6100/10000 (61%)\n",
            "\n",
            "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.231686\n",
            "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 1.307930\n",
            "\n",
            "Test set: Average loss: 1.1379, Accuracy: 6040/10000 (60%)\n",
            "\n",
            "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.082128\n",
            "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1.184104\n",
            "\n",
            "Test set: Average loss: 1.0696, Accuracy: 6308/10000 (63%)\n",
            "\n",
            "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.199576\n",
            "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1.003612\n",
            "\n",
            "Test set: Average loss: 1.0985, Accuracy: 6122/10000 (61%)\n",
            "\n",
            "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.055004\n",
            "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1.198592\n",
            "\n",
            "Test set: Average loss: 1.0885, Accuracy: 6243/10000 (62%)\n",
            "\n",
            "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.156653\n",
            "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1.080394\n",
            "\n",
            "Test set: Average loss: 1.0425, Accuracy: 6426/10000 (64%)\n",
            "\n",
            "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.136955\n",
            "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1.198087\n",
            "\n",
            "Test set: Average loss: 1.0468, Accuracy: 6487/10000 (65%)\n",
            "\n",
            "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.900573\n",
            "Train Epoch: 22 [32000/50000 (64%)]\tLoss: 0.975735\n",
            "\n",
            "Test set: Average loss: 1.0584, Accuracy: 6338/10000 (63%)\n",
            "\n",
            "Train Epoch: 23 [0/50000 (0%)]\tLoss: 1.033908\n"
          ]
        }
      ]
    }
  ]
}